
### Linear Factor Model 

선형 인자 모델은 선형 연산을 이용하여 관찰한 데이터를 인자로 변환하는 방법이다. 

여기서 인자란 현상으로 나타나지 않는 즉 관찰되지 않는 변수를 뜻한다. 

![Untitled (11)](https://github.com/IyLias/i-am-a-developer/assets/48081162/745656c9-d3ed-4ba8-b746-367a315c428e)


예를 들어 위의 그림에서 $z$ 가 인자에 해당한다.  인자를 잠복변수 또는 은닉변수라고도 한다. 

선형 인자 모델은 선형 연산을 사용하므로 행렬 곱으로 인코딩과 디코딩 과정을 표현할 수 있다. 

![Untitled (12)](https://github.com/IyLias/i-am-a-developer/assets/48081162/d8890f0a-1955-4031-8d13-319d06682386)


여기서 $\alpha$ 는 데이터를 원점으로 이동하거나 잡음을 추가하는 등의 역할을 한다. 

인자 $z$ 와 추가 항 $\alpha$ 의 성질에 따라 여러 가지 모델이 가능하다. 

- $z$ 가 확률 개념을 가지지 않고 $\alpha$ 항에 아무 것도 없다면 ⇒ PCA

- PCA에 확률 개념, 즉 $z$ 와 $\alpha$ 가 가우시안 분포를 따른다고 하면 확률 PCA

- $z$ 가 비가우시안 분포를 따른다고 가정하는 ICA

### PCA : Principal Component Analysis  주성분 분석

주성분 분석은 데이터를 원점 중심으로 옮겨 놓는 일부터 시작한다. 

![Untitled (13)](https://github.com/IyLias/i-am-a-developer/assets/48081162/f185758e-9282-4fa9-b3c1-c3748aaed385)


주성분 분석에서는 $z$ 에 확률 개념이 없고 $\alpha$ 는 필요가 없어 생략한다. 그렇게 식을 적으면 

$z = W^TX$ 가 된다. 

![Untitled (14)](https://github.com/IyLias/i-am-a-developer/assets/48081162/efbf4e5a-e450-4846-96d5-df9e01e70d26)


결론적으로 주성분 분석이 하는 작업은 변환 행렬 $W$를 통해 $d$ 차원의 훈련 집합 샘플 $X$를  $q$ 차원의 $z$ 로 변환하는 것이다. 

주성분 분석 PCA 에 의한 변환은 정보 손실을 일으키는데 정보 손실이 적을 수록 좋은 축이다. (축으로 변환했을 때 겹쳐서 중복되면 정보 손실이 발생한다.)

따라서 PCA의 목적은 정보 손실을 최소화하면서 저차원으로 변환하는 것이다,.

이를 수학적으로 서술하면 주성분 분석은 변환된 새로운 공간에서 샘플이 얼마나 퍼져 있는지를 따져 정보 손실을 측정한다. 즉 변환된 훈련집합 $Z = \{ z_1,z_2,...,z_n\}$ 의 분산이 크면 클수록 정보 손실이 작다고 판단한다. 

그래서 PCA의 최적화 문제를 다음과 같이 서술한다. 

$Z = \{z_1,z_2,...,z_n\}$ 의 분산을 최대화하는 $q$ 개의 축, $u_1,u_2,...,u_q$ 를 찾아라. 

### ICA : Independent Component Analysis  독립 성분 분석

실제 세계에서는 여러 독립적인 신호가 섞여 나타난다. 주어진 혼합 신호로부터 원래 각각의 신호로 복원하는 문제를 블라인드 원음 분리 문제 라고 하며 이는 독립 성분 분석 ICA 로 해결한다. 

원래 신호를 $z_1(t)$ 와 $z_2(t)$ 라 하고 측정된 혼합 신호를 $x_1(t)$ 와 $x_2(t)$ 로 표기한다. 

![Untitled (15)](https://github.com/IyLias/i-am-a-developer/assets/48081162/dba4d728-26fc-4178-8038-743975fc70b0)


$t$ 순간에 획득한 $x_t = (x_1(t), x_2(t))^T$ 를 훈련 샘플로 하며 훈련 집합 $X = \{x_1,x_2,...,x_n\}$ 이다. 

결국 블라인드 원음 분리 문제는 훈련 집합 $X$ 로 부터 $Z = \{z_1,z_2,...,z_n\}$ 을 찾는 문제이다. 

소리의 물리법칙에 따라 혼합 신호 $x$ 를 원래 신호 $z$ 의 선형 결합으로 표현할 수 있다. 

$x_1 = a_{11}z_1 + a_{12}z_2$

$x_2 = a_{21}z_1 + a_{22}z_2$  

이를 행렬로 바꿔 쓰면 $X = AZ$ 이며 최종적으로 구할 것은 행렬  $A$ 가 된다. 

행렬 $A$를 구하면 $A$ 의 역행렬을 $X$에 곱해서 원음 $Z$ 를 구할 수 있기 때문이다. 

### 

### ICA의 가정

- 독립성 가정

각각의 신호는 독립적이라는 가정을 한다. 

![Untitled (16)](https://github.com/IyLias/i-am-a-developer/assets/48081162/8cd94d54-5a26-4c9a-937b-f58f16959b98)


- 비가우시안 가정
    
    
    독립성만 가정한다고 해결되지는 않는데 원래 신호 $z_i$ 가 비가우시안이어야 한다는 비가우시안 가정이 필요하다. 
    
    원래 신호 $z_i$ 가 가우시안 분포를 따르면 $z_i$ 의 선형 결합으로 만들어진 합성 신호 $x_i$ 도 가우시안 분포를 따르게 되며 $x_1,x_2,...,x_d$ 의 결합분포 $P(X)$ 도 가우시안이 되기 때문이다.  이렇게 되면 훈련집합 $X$ 가 아래와 같은 분포를 띄게 되며 원래 신호를 복원하기가 어려워진다. 
    
    반면 비가우시안이라면 오른쪽처럼 두 신호를 분리할 가능성이 생긴다. 
    

![Untitled (17)](https://github.com/IyLias/i-am-a-developer/assets/48081162/4336710b-fe67-442b-9e80-74d18ab9313d)


그래서 $ICA$ 는 원래 신호의 비가우시안 정도를 최대화하는 가중치를 구하는 전략을 사용한다.

## 

## PCA 와 ICA 의 비교

- PCA는 가우시안과 비상관을 가정하지만 ICA는 비가우시안과 독립성을 가정한다.

- PCA는 2차 모멘트까지 사용하지만 ICA는 4차 모멘트까지 사용한다.

- PCA로 찾은 축은 서로 수직이지만 ICA로 찾은 축은 수직이 아니다.

- PCA는 차원 축소 문제를 다루고 ICA 는 블라인드 원음 분리 문제를 푼다.

## 

## 희소 코딩

조각 영상을 여러 개의 기저 벡터 선형 결합으로 표현한다. 

![Untitled (18)](https://github.com/IyLias/i-am-a-developer/assets/48081162/65a712a0-a28d-4612-86ce-c0ee9627f8a4)


희소 코딩은 최적의 사전과 최적의 희소 코드를 알아내는 것이다. 즉 최적의 basis를 구하는 것 = 최적의 계수 집합을 구하는 것이다.
