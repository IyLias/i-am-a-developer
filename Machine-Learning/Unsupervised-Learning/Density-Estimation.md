어떤 점 $x$ 에서 데이터가 발생할 확률, 즉 확률밀도함수 $P(x)$를 구하는 문제가 밀도 추정이다. 

## Method 1 :  Histogram Estimation

축을 여러 구간으로 나누어 특징 공간을 칸의 집합으로 분할한 다음 각각의 칸에 있는 샘플의 빈도를 세는 것이다. 

$bin(x)$ 를 점 $x$ 가 놓인 칸에 있는 샘플의 개수, $n$ 을 훈련집합에 있는 샘플의 수라고 하면 

$P(x) = \frac{bin(x)}{n}$ 이다. 

단순한 방법이지만 문제점들이 존재한다. 

- 확률밀도함수 $P(X)$ 가 매끄럽지 못하고  계단 모양을 띄게 된다.

- 칸의 크기와 위치에 민감하다. ⇒ 칸을 키우거나 줄이면 결과가 크게 달라지게 된다.

## Method 2 :  Kernel Density Estimation

식에서 $K$ 는 표준커널함수이고 $K_h$ 는 크기 변환된 커널 함수이다. 

![Untitled (4)](https://github.com/IyLias/i-am-a-developer/assets/48081162/1315ac94-35e5-467d-b8f3-5d21d91a5403)

커널에서 $h$ 값이 클수록 뭐 1보다 클수록 높이는 낮아지고 넓은 형태가 될 것이고 

반대로 1보다 작아진다면 높이가 높고 간격은 짧은 형태가 될 것이다.

커널의 대역폭 $h$는 사용자가 설정해야 하는 하이퍼 매개변수이다. ⇒ 적절하게 설정해줘야 한다. 

- $h$ 가 너무 작다면, 즉 좁고 높은 가우시안을 사용한다면 뾰족뾰족한 함수 모양이 나온다.
- $h$ 가 너무 크다면, 즉 넓고 낮은 가우시안을 사용한다면 뭉개진 함수가 나온다.

### 커널 밀도 추정법의 문제점

### 

- 훈련집합의 샘플을 모두 저장하고 있어야 하는 메모리 기반 방법이라 훈련집합의 크기가 크면 필요한 메모리가 너무 많아지는 문제가 발생한다.

- 새로운 샘플이 주어질 때마다 아래 식을 다시 계산해야하는 문제가 있다.

![Untitled (5)](https://github.com/IyLias/i-am-a-developer/assets/48081162/76a42008-c16e-42dd-92e5-57e369df300c)


- 특징 공간이 고차원일수록 데이터가 희소한 문제가 생긴다.

결론: 커널 밀도 추정법은 이론 상으로는 아무리 복잡한 모양의 분포라도 표현할  수 있다. 하지만 데이터의 희소성과 고차원성 등의 현실적인 문제로 응용 범위에 제한을 받을 수 밖에 없다. 

## Method 3:  Gausian Method

이 방식은 데이터가 일정한 모양의 분포를 따른다는 가정하에 확률분포를 추정한다. 

가우시안 방식은 훈련집합으로부터 평균 벡터 $\mu$ 와 공분산 행렬 $\sum$ 을 한 번 계산하면 이후부터는 훈련집합이 없어도 확률분포를 계산할 수 있다. 

![Untitled (6)](https://github.com/IyLias/i-am-a-developer/assets/48081162/83ba0abb-30d9-4efe-98b9-85784f0246e0)


이 때 평균 벡터와 공분산 행렬의 계산은 식 6.9를 이용하여 $\theta(nd)$ 만에 마치게 된다. 

그리고 데이터가 $d$ 차원이라면 $\mu$ 는 $d$ 개, $\sum$ 은 $d^2$ 개의 요소를 가지므로 총 $d^2 + d$ 개의 값만 저장하면 된다. 

![Untitled (7)](https://github.com/IyLias/i-am-a-developer/assets/48081162/db102026-348e-45d0-913b-fb30e253cc0e)



왼쪽은 비교적 정확하게 추정된 모습이다. 

반면 오른쪽은 오차가 크다. 가우시안이 가장 높은 확률을 출력할 평균점 $\mu$ 에서는 샘플이 매우 희소하게 분포하므로 실제로는 아주 낮은 확률을 가질 것이다.

애초에 그럴 수 밖에 없는 게 오른 쪽 데이터 셋은 하나의 가우시안으로 처리하기 어려운 구조로 분포해있다. 

그래서 등장하는 개념인 

### 

### 가우시안 혼합

말 그대로 여러 개의 가우시안을 혼합하여 확률밀도를 표현하는 것이다. 

이제 이를 일반화하여 $k$ 개의 가우시안을 선형 결합 형태로 표현한 확률밀도함수 $P(X)$를 생각해보자. 

$P(X)$=$\sum\limits_{j=1}^k$ $\pi_jN(X;\mu_j, \sum_j)$ 

각각의 가우시안은 $N(X;\mu_j, \sum_j)$ 로 표현하며 요소 분포라 부르고 앞의 계수 $\pi_j$는 가중치 역할을 하게 되며 혼합 계수 라 한다. 

이 때 혼합 계수는 $0 \le \pi_j \le 1$ 과 $\sum_{j=1}^k \pi_j = 1$ 을 만족한다. 

그러면 이제 이 문제를 정리해보자. 

우선 주어진 데이터를 훈련집합 $X = \{x_1, x_2, ..., x_n\}$ 과 가우시안 개수 $k$ 라 할 때

우리가 추정해야 할 매개변수 집합 $\theta = \{ \pi = (\pi_1, \pi_2, ..., \pi_k), (\mu_1, \sum_1), (\mu_2, \sum_2), ...,(\mu_k, \sum_k)\}$ 이다.

이를 최대 우도를 이용한 최적화 문제로 공식화 하면 다음과 같다. 

![Untitled (8)](https://github.com/IyLias/i-am-a-developer/assets/48081162/bf80c3d2-6b63-48f3-8dd4-f590c8583293)


 

그래서 최종 목표는 저 값을 최대로 하는 매개변수 집합 $\theta$ 를 찾는 것이다. 즉 주어진 $X$ 가 발생할 가능성이 가장 큰 $\theta$ 를 찾는 문제이다.  처음에 $\theta$ 는 아무 것도 모르므로 난수로 설정한다. 

최종 식을 해결하는 과정은 EM 알고리즘을 활용하게 된다. 

2개의 가우시안을 통한 작업을 예로 들자..  처음에는 매개변수 값들이 난수로 되어 있다,. 

1. 2개의 가우시안이 자신에 속하는 샘플이 어느 것인지 알면
2. 소속된 샘플을 이용하여 평균과 공분산 행렬을 다시 추정하여 자신을 개선한다. 
3. 가우시안이 자신의 매개변수를 개선하면 개선된 가우시안으로 샘플의 소속 정보를 개선할 수 있다. 
4. 다시 개선된 소속 정보는 가우시안을 더 정확하게 개선한다. 

이를 코드화 한게 아래와 같다. 

![Untitled (9)](https://github.com/IyLias/i-am-a-developer/assets/48081162/36700a51-56b3-4bd7-a54e-17bdc7a97ecf)



EM 알고리즘은 일종의 최대 우도 추정법인데 주로 불완전한 데이터, 즉 손실 정보가 포함된 데이터가 주어진 경우에 적용한다.

